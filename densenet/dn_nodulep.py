    # -*- coding: utf-8 -*-
"""DC_histopathology_capsule.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1Xw5Qs2ldxgB3-8s9h2bWQtuZ2-aaXgP3

    run-2
"""

    # from google.colab import drive
    # drive.mount('/content/drive')

"""## Introduction


"""
for i in range(10):
    import os
    import numpy as np



    import numpy as np
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    # import tensorflow_addons as tfa
    # import keras_tuner as kt

    # from google.colab import drive


    # import shutil

    # zip_file_path = '/content/gdrive/MyDrive/DC/Image_files/FA_capsule.zip'  # Replace with the actual path to your ZIP file
    # destination_path = '/content/FA_Capsule.zip'  # Destination path in Colab

    # shutil.copyfile(zip_file_path, destination_path)

    # !unzip FA_Capsule.zip

    # zip_file_path = '/content/gdrive/MyDrive/DC/Image_files/NIFTP_capsule.zip'  # Replace with the actual path to your ZIP file
    # destination_path = '/content/NIFTP_capsule.zip'  # Destination path in Colab

    # shutil.copyfile(zip_file_path, destination_path)

    # !unzip NIFTP_capsule.zip

    # import shutil

    # zip_file_path = '/content/gdrive/MyDrive/DC/Image_files/PTC_capsule.zip'  # Replace with the actual path to your ZIP file
    # destination_path = '/content/PTC_capsule.zip'  # Destination path in Colab

    # shutil.copyfile(zip_file_path, destination_path)

    # !unzip PTC_capsule.zip

    path = "/workspace/storage"

    FA_nodule=os.listdir(path+"/content/FA_nodule")
    print("Total images in FA_capsule ",len(FA_nodule))
    NIFTP_nodule=os.listdir(path+"/content/NIFTP nodule")
    print("Total images in NIFTP_capsule ",len(NIFTP_nodule))
    PTC_nodule=os.listdir(path+"/content/PTC_nodule")
    print("Total images in PTC_capsule ",len(PTC_nodule))

    # FAC_nodule=os.listdir("/content/gdrive/MyDrive/dc/FAnodule")
    # print("Total images in FA_nodule ",len(FAC_capsule))
    # NIFTP_nodule=os.listdir("/content/gdrive/MyDrive/dc/NIFTPnodule")
    # print("Total images in NIFTP_nodule ",len(NIFTP_capsule))
    # PTC_nodule=os.listdir("/content/gdrive/MyDrive/dc/PTCnodule")
    # print("Total images in PTC_nodule ",len(PTC_capsule))

    patient_dir = []
    patient_ptc=[]
    patient_fa=[]
    patient_niftp=[]

    root_dir_ptc=os.listdir(path+"/content/PTC_nodule/")
    for dir in root_dir_ptc:
        patient_ptc.append(dir.split("_")[0])

    for item in patient_ptc:
        if item not in patient_dir:
            patient_dir.append(item)

    root_dir_fa=os.listdir(path+"/content/FA_nodule/")
    for dir in root_dir_fa:
        patient_fa.append(dir.split("_")[0])

    for item in patient_fa:
        if item not in patient_dir:
            patient_dir.append(item)

    root_dir_niftp=os.listdir(path+"/content/NIFTP nodule")
    for dir in root_dir_niftp:
        patient_niftp.append(dir.split("_")[0])

    for item in patient_niftp:
        if item not in patient_dir:
            patient_dir.append(item)

    print("Total unique patient ID of FA : ", len(np.unique(patient_fa)))
    print("Total unique patient ID of PTC : ", len(np.unique(patient_ptc)))
    print("Total unique patient ID of NIFTP : ", len(np.unique(patient_niftp)))
    print("Total patient ID including FA,PTC,NIFTP : ", len(patient_dir))


    from collections import Counter
    common_ids=[item for item, count in Counter(patient_dir).items() if count > 1]

    for id in common_ids:
        patient_dir.remove(id)
        patient_dir.remove(id)

    len_patient=len(patient_dir)
    print(len_patient)

    # print("Training set will contain dataset of ", int(0.8*56)," patients")

    len_train=int(0.7*len_patient)
    len_val=int(0.1*len_patient)
    len_test=len(patient_dir)-(len_train+len_val)
    print("Training set will contain dataset of ", int(0.7*len_patient)," patients")
    print("Validation set will contain dataset of ", int(0.1*len_patient)," patients")

    # import random
    # train_id = random.sample(patient_dir, 44)

    while True:
        test_id = np.random.choice(patient_dir, size=len_test, replace=False)
        train_id=np.setdiff1d(patient_dir,test_id)

        if len(np.intersect1d(train_id,patient_fa))>0 and len(np.intersect1d(train_id,patient_niftp))>0 and len(np.intersect1d(train_id, patient_ptc))>0 and len(np.intersect1d(test_id,patient_fa))>0 and len(np.intersect1d(test_id,patient_niftp))>0 and len(np.intersect1d(test_id, patient_ptc))>0:
            
            break

    while True:
        val_id= np.random.choice(train_id, size=len_val, replace=False)
        train_id=np.setdiff1d(train_id, val_id)
        if len(np.intersect1d(train_id,patient_fa))>0 and len(np.intersect1d(train_id,patient_niftp))>0 and len(np.intersect1d(train_id, patient_ptc))>0 and len(np.intersect1d(val_id,patient_fa))>0 and len(np.intersect1d(val_id,patient_niftp))>0 and len(np.intersect1d(val_id, patient_ptc))>0:

            break
        else:
            train_id = np.append(train_id, val_id)

    print(train_id, len(train_id), val_id, len(val_id), test_id, len(test_id))

    results_dict={}
    results_dict['training_patient_ids']=str(train_id.tolist())
    results_dict['validation_patient_ids']=str(val_id.tolist())
    results_dict['testing_patient_ids']=str(test_id.tolist())

    # print(FAC_nodule[0])

    import glob
    fac = glob.glob('/workspace/storage/results/densenet/FAcapsule/*.*')
    niftp = glob.glob('/workspace/storage/results/densenet/NIFTPcapsule/*.*')
    ptc= glob.glob('/workspace/storage/results/densenet/PTCcapsule/*.*')

    X_train=[]
    X_val=[]
    X_test=[]
    ytrain=[]
    yval=[]
    ytest=[]

    for i in FA_nodule:
        image=tf.keras.preprocessing.image.load_img(path+"/content/FA_nodule/"+i, color_mode='rgb',
        target_size= (224, 224))
        image=np.array(image)
        if i.split("_")[0] in train_id:
            X_train.append(image)
            ytrain.append(0)
        elif i.split("_")[0] in val_id:
            X_val.append(image)
            yval.append(0)

        elif i.split("_")[0] in test_id:
            X_test.append(image)
            ytest.append(0)

    for i in NIFTP_nodule:
        image=tf.keras.preprocessing.image.load_img(path+"/content/NIFTP nodule/"+i, color_mode='rgb',
        target_size= (224, 224))
        image=np.array(image)
        if i.split("_")[0] in train_id:
            X_train.append(image)
            ytrain.append(1)
        elif i.split("_")[0] in val_id:
            X_val.append(image)
            yval.append(1)
        elif i.split("_")[0] in test_id:
            X_test.append(image)
            ytest.append(1)

    for i in PTC_nodule:
        image=tf.keras.preprocessing.image.load_img(path+"/content/PTC_nodule/"+i, color_mode='rgb',
        target_size= (224, 224))
        image=np.array(image)
        if i.split("_")[0] in train_id:
            X_train.append(image)
            ytrain.append(2)
        elif i.split("_")[0] in val_id:
            X_val.append(image)
            yval.append(2)
        elif i.split("_")[0] in test_id:
            X_test.append(image)
            ytest.append(2)


    X_train=np.array(X_train)
    X_val=np.array(X_val)
    X_test=np.array(X_test)
    ytrain=np.array(ytrain)
    yval=np.array(yval)
    ytest=np.array(ytest)


    #shuffling the data
    shuffler_train=np.random.permutation(X_train.shape[0])
    shuffler_val=np.random.permutation(X_val.shape[0])
    shuffler_test=np.random.permutation(X_test.shape[0])
    X_train=X_train[shuffler_train]
    X_val=X_val[shuffler_val]
    X_test=X_test[shuffler_test]
    ytrain=ytrain[shuffler_train]
    yval=yval[shuffler_val]
    ytest=ytest[shuffler_test]

    print(X_train.shape)
    print(X_val.shape)
    print(X_test.shape)
    print(ytrain.shape)
    print(yval.shape)
    print(ytest.shape)


    """## Setup

    ## Prepare the data
    """

    num_classes = 3
    input_shape = (224, 224, 3)

    """## Configure the hyperparameters"""

    learning_rate = 0.001 
    weight_decay = 0.0001 
    batch_size = 128 
    num_epochs = 100 
    image_size = 224  # We'll resize input images to this size
    # Size of the patches to be extract from the input images
    # Size of the dense layers of the final classifier

    """## Use data augmentation"""

    data_augmentation = keras.Sequential(
        [
            # layers.Resizing(image_size, image_size),
            layers.RandomFlip("horizontal"),
            layers.RandomRotation(factor=0.02),
            layers.RandomZoom(
                height_factor=0.2, width_factor=0.2
            ),
        ],
        name="data_augmentation",
    )
    # Compute the mean and the variance of the training data for normalization.

    X_train = tf.convert_to_tensor(X_train)
    ytrain = tf.convert_to_tensor(ytrain)
    X_train = data_augmentation(X_train)

    import tensorflow as tf
    from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Dense
    from tensorflow.keras.layers import AvgPool2D, GlobalAveragePooling2D, MaxPool2D
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import ReLU, concatenate
    import tensorflow.keras.backend as K
    # Creating Densenet121
    def densenet(input_shape, n_classes, filters = 32):

        #batch norm + relu + conv
        def bn_rl_conv(x,filters,kernel=1,strides=1):

            x = BatchNormalization()(x)
            x = ReLU()(x)
            x = Conv2D(filters, kernel, strides=strides,padding = 'same')(x)
            return x

        def dense_block(x, repetition):

            for _ in range(repetition):
                y = bn_rl_conv(x, 4*filters)
                y = bn_rl_conv(y, filters, 3)
                x = concatenate([y,x])
            return x

        def transition_layer(x):
            x = bn_rl_conv(x, K.int_shape(x)[-1] //2 )
            x = AvgPool2D(2, strides = 2, padding = 'same')(x)
            return x

        input = Input (input_shape)
        x = Conv2D(64, 7, strides = 2, padding = 'same')(input)
        x = MaxPool2D(3, strides = 2, padding = 'same')(x)

        for repetition in [6,12,24,16]:
            d = dense_block(x, repetition)
            x = transition_layer(d)

        x = GlobalAveragePooling2D()(d)
        output = Dense(n_classes, activation = 'softmax')(x)

        model = Model(input, output)
        return model
    
    input_shape = 224, 224, 3
    n_classes = 3
    # model = densenet(input_shape,n_classes)
    # model.summary()

    """## Compile, train, and evaluate the mode"""

    def run_experiment(model):
        optimizer = tf.keras.optimizers.Adam(
            learning_rate=learning_rate, decay=weight_decay
        )

        model.compile(
            optimizer=optimizer,
            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=[
                keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
            ],
        )
        # model.compile(loss='categorical_crossentropy',
        #           optimizer=optimizer,
        #           metrics=['accuracy'])

        # tuner = kt.Hyperband(create_vit_classifier,
        #                      objective='val_accuracy',
        #                      max_epochs=10,
        #                      factor=3,
        #                      )
        # earlystop_callback= keras.callbacks.EarlyStopping(
        #     monitor='val_accuracy',
        #     min_delta=0.001,
        #     patience=3,
        #     verbose=1,
        # )

        checkpoint_filepath = "/tmp/checkpoint"
        checkpoint_callback = keras.callbacks.ModelCheckpoint(
            checkpoint_filepath,
            monitor="val_accuracy",
            save_best_only=True,
            save_weights_only=True,
        )

        # tuner.search(X_train, ytrain, epochs=100, batch_size=batch_size, validation_data=(X_val, yval), callbacks=[checkpoint_callback, earlystop_callback])

        # Get the optimal hyperparameters
        # best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]
        # print(best_hps)
        # print("hello")

        # model = tuner.hypermodel.build(best_hps)

        history = model.fit(
            x=X_train,
            y=ytrain,
            batch_size=batch_size,
            epochs=100,
            validation_data=(X_val, yval),
            callbacks=[checkpoint_callback],
        )

        model.load_weights(checkpoint_filepath)
        _, accuracy = model.evaluate(X_test, ytest)
        yt=model.predict(X_test)


        pred=[]
        for i in range(len(yt)):
            d=np.where(yt[i]==max(yt[i]))
            pred.append(d[0][0])
        print(f"Test accuracy: {round(accuracy * 100, 2)}%")
        # print(f"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%")

        return history,pred,yt

    # ds = tf.keras.applications.densenet.DenseNet121(
    #     include_top=True,
    #     weights='imagenet',
    #     input_tensor= tf.convert_to_tensor(X_train),
    #     input_shape=(224,224,3),
    #     pooling=None,
    #     classes=1000,
    #     classifier_activation='softmax'
    # )
    ds_classifier = densenet(input_shape,n_classes)


    history,pred,prob = run_experiment(ds_classifier)
    print(history.history.keys())

    pred

    ytest

    prob.shape

    acc=0
    for i in range(len(pred)):
        if (pred[i]==ytest[i]):
            acc+=1
    print(acc/len(pred)*100)

    import matplotlib.pyplot as plt
    from datetime import datetime

    now= datetime.now()
    dt_string = now.strftime("%d_%m_%Y %H_%M_%S")

    print(history.history.keys())
    # summarize history for accuracy
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    # plt.show()
    plt.savefig('/workspace/storage/results/densenet/'+dt_string+' nodule_accuracy')
    plt.show()
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    # plt.show()
    plt.savefig('/workspace/storage/results/densenet/'+dt_string+' nodle_loss')
    plt.show()

    """seeding , au roc class wise, try other transformer deit recent ones, con next, common patient , VIT large, diff hyperpara , 217A,"""

    from sklearn.metrics import classification_report
    target_names = ['FA', 'NIFTP', 'PTC']
    metric_report=classification_report(ytest, pred, target_names=target_names, output_dict=True)
    print(classification_report(ytest, pred, target_names=target_names))

    results_dict['Class-wise Precision']="FA - "+str(metric_report['FA']['precision'])+", "+"NIFTP - "+str(metric_report['NIFTP']['precision'])+", "+"PTC - "+str(metric_report['PTC']['precision'])+", "
    results_dict['Macro-averaged Precision']=str(metric_report['macro avg']['precision'])
    results_dict['Weighted-averaged Precision']=str(metric_report['weighted avg']['precision'])

    results_dict['Class-wise recall']="FA - "+str(metric_report['FA']['recall'])+", "+"NIFTP - "+str(metric_report['NIFTP']['recall'])+", "+"PTC - "+str(metric_report['PTC']['recall'])+", "
    results_dict['Macro-averaged recall']=str(metric_report['macro avg']['recall'])
    results_dict['Weighted-averaged recall']=str(metric_report['weighted avg']['recall'])

    results_dict['Class-wise f1-score']="FA - "+str(metric_report['FA']['f1-score'])+", "+"NIFTP - "+str(metric_report['NIFTP']['f1-score'])+", "+"PTC - "+str(metric_report['PTC']['f1-score'])+", "
    results_dict['Macro-averaged f1-score']=str(metric_report['macro avg']['f1-score'])
    results_dict['Weighted-averaged f1-score']=str(metric_report['weighted avg']['f1-score'])

    print(results_dict)

    from sklearn.metrics import confusion_matrix
    model_conf = confusion_matrix(ytest, pred)
    print(model_conf)

    X_test.shape

    X_test2=X_test

    import pandas as pd
    df = pd.DataFrame(X_test2.tolist())

    df

    import seaborn as sns
    from sklearn.metrics import roc_auc_score
    def calculate_tpr_fpr(y_real, y_pred):
        '''
        Calculates the True Positive Rate (tpr) and the True Negative Rate (fpr) based on real and predicted observations

        Args:
            y_real: The list or series with the real classes
            y_pred: The list or series with the predicted classes

        Returns:
            tpr: The True Positive Rate of the classifier
            fpr: The False Positive Rate of the classifier
        '''

        # Calculates the confusion matrix and recover each element
        cm = confusion_matrix(y_real, y_pred)
        TN = cm[0, 0]
        FP = cm[0, 1]
        FN = cm[1, 0]
        TP = cm[1, 1]

        # Calculates tpr and fpr
        tpr =  TP/(TP + FN) # sensitivity - true positive rate
        fpr = 1 - TN/(TN+FP) # 1-specificity - false positive rate

        return tpr, fpr

    def get_all_roc_coordinates(y_real, y_proba):
        '''
        Calculates all the ROC Curve coordinates (tpr and fpr) by considering each point as a threshold for the predicion of the class.

        Args:
            y_real: The list or series with the real classes.
            y_proba: The array with the probabilities for each class, obtained by using the `.predict_proba()` method.

        Returns:
            tpr_list: The list of TPRs representing each threshold.
            fpr_list: The list of FPRs representing each threshold.
        '''
        tpr_list = [0]
        fpr_list = [0]
        for i in range(len(y_proba)):
            threshold = y_proba[i]
            y_pred = y_proba >= threshold
            tpr, fpr = calculate_tpr_fpr(y_real, y_pred)
            tpr_list.append(tpr)
            fpr_list.append(fpr)
        return tpr_list, fpr_list


    def plot_roc_curve(tpr, fpr, scatter = True, ax = None):
        '''
        Plots the ROC Curve by using the list of coordinates (tpr and fpr).

        Args:
            tpr: The list of TPRs representing each coordinate.
            fpr: The list of FPRs representing each coordinate.
            scatter: When True, the points used on the calculation will be plotted with the line (default = True).
        '''
        # if ax == None:
        #     plt.figure(figsize = (5, 5))
        #     ax = plt.axes()

        # if scatter:
        #     sns.scatterplot(x = fpr, y = tpr, ax = ax)
        # sns.lineplot(x = fpr, y = tpr, ax = ax)
        # sns.lineplot(x = [0, 1], y = [0, 1], color = 'green', ax = ax)
        # plt.xlim(-0.05, 1.05)
        # plt.ylim(-0.05, 1.05)
        # plt.xlabel("False Positive Rate")
        # plt.ylabel("True Positive Rate")

    # plt.figure(figsize = (12, 8))
    bins = [i/20 for i in range(20)] + [1]
    classes = [0,1,2]
    roc_auc_ovr = {}
    for i in range(len(classes)):
        # Gets the class
        c = classes[i]

        # Prepares an auxiliar dataframe to help with the plots
        df_aux = df.copy()
        # df={}
        # print(classes[i],ytest[0])
        df_aux['class'] = [1 if y == c else 0 for y in ytest]
        df_aux['prob'] = prob[:, i]
        df_aux = df_aux.reset_index(drop = True)

        # Plots the probability distribution for the class and the rest
        # ax = plt.subplot(2, 3, i+1)
        # sns.histplot(x = "prob", data = df_aux, hue = 'class', color = 'b', ax = ax, bins = bins)
        # ax.set_title(c)
        # ax.legend([f"Class: {c}", "Rest"])
        # ax.set_xlabel(f"P(x = {c})")

        # Calculates the ROC Coordinates and plots the ROC Curves
        # ax_bottom = plt.subplot(2, 3, i+4)
        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])
        # plot_roc_curve(tpr, fpr, scatter = False)a
        # ax_bottom.set_title("ROC Curve OvR")

        # Calculates the ROC AUC OvR
        roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])
    plt.tight_layout()

    # Displays the ROC AUC for each class
    avg_roc_auc = 0
    i = 0
    class_dict={0:'FA',1:'NIFTP', 2:'PTC'}
    roc_auc_str=''
    for k in roc_auc_ovr:
        avg_roc_auc += roc_auc_ovr[k]
        i += 1
        print(f"{k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}")
        roc_auc_str=roc_auc_str+class_dict[k]+' - '+f"{roc_auc_ovr[k]:.4f}"+"  "

    results_dict['Class-wise ROC_AUC score']=roc_auc_str
    print(f"average ROC AUC OvR: {avg_roc_auc/i:.4f}")
    results_dict['Average ROC_AUC score']=f"{avg_roc_auc/i:.4f}"
    print(results_dict)

    macro_roc_auc_func = roc_auc_score(ytest, prob, labels = classes, multi_class = 'ovr', average = 'macro')
    print('macro average roc_auc score from function - ', macro_roc_auc_func)
    weighted_roc_auc_func = roc_auc_score(ytest, prob, labels = classes, multi_class = 'ovr', average = 'weighted')
    print('weighted average roc_auc score from function - ', weighted_roc_auc_func)
    macro_ovo_roc_auc_func = roc_auc_score(ytest, prob, labels = classes, multi_class = 'ovo', average = 'macro')
    print('macro average ovo roc_auc score from function - ', macro_ovo_roc_auc_func)
    weighted_ovo_roc_auc_func = roc_auc_score(ytest, prob, labels = classes, multi_class = 'ovo', average = 'weighted')
    print('weighted average ovo roc_auc score from function - ', weighted_ovo_roc_auc_func)

    results_dict['Macro Average ROC_AUC score'] = macro_roc_auc_func
    results_dict['Weighted Average ROC_AUC score'] = weighted_roc_auc_func
    results_dict['Macro Average ovo ROC_AUC score'] = macro_roc_auc_func
    results_dict['Weighted Average ovo ROC_AUC score'] = weighted_roc_auc_func


    prob.shape

    # Compares with sklearn (average only)
    # "Macro" average = unweighted mean
    # roc_auc_score(ytest, prob, labels = classes, multi_class = 'ovr', average = 'macro')

    myArray = prob

    #Transform data
    normalizedArray = []

    for row in range(0, len(myArray)):
        list = []
        Min =  min(myArray[row])
        Max = max(myArray[row])

        for element in myArray[row]:
            list.append(  float(element-Min)/float(Max- Min) )

        normalizedArray.append(list)

    #Normalize to 1
    newArray = []

    for row in range(0, len(normalizedArray)):
        list = [x / sum(normalizedArray[row]) for x in normalizedArray[row]]
        newArray.append(list)

    newArray



    # Compares with sklearn (average only)
    # "Macro" average = unweighted mean

    roc_auc_score(ytest, newArray, labels = classes, multi_class = 'ovr', average = 'macro')

    import seaborn as sns
    from sklearn.metrics import roc_auc_score
    def calculate_tpr_fpr(y_real, y_pred):
        '''
        Calculates the True Positive Rate (tpr) and the True Negative Rate (fpr) based on real and predicted observations

        Args:
            y_real: The list or series with the real classes
            y_pred: The list or series with the predicted classes

        Returns:
            tpr: The True Positive Rate of the classifier
            fpr: The False Positive Rate of the classifier
        '''

        # Calculates the confusion matrix and recover each element
        cm = confusion_matrix(y_real, y_pred)
        TN = cm[0, 0]
        FP = cm[0, 1]
        FN = cm[1, 0]
        TP = cm[1, 1]

        # Calculates tpr and fpr
        tpr =  TP/(TP + FN) # sensitivity - true positive rate
        fpr = 1 - TN/(TN+FP) # 1-specificity - false positive rate

        return tpr, fpr

    def get_all_roc_coordinates(y_real, y_proba):
        '''
        Calculates all the ROC Curve coordinates (tpr and fpr) by considering each point as a threshold for the predicion of the class.

        Args:
            y_real: The list or series with the real classes.
            y_proba: The array with the probabilities for each class, obtained by using the `.predict_proba()` method.

        Returns:
            tpr_list: The list of TPRs representing each threshold.
            fpr_list: The list of FPRs representing each threshold.
        '''
        tpr_list = [0]
        fpr_list = [0]
        for i in range(len(y_proba)):
            threshold = y_proba[i]
            y_pred = y_proba >= threshold
            tpr, fpr = calculate_tpr_fpr(y_real, y_pred)
            tpr_list.append(tpr)
            fpr_list.append(fpr)
        return tpr_list, fpr_list


    def plot_roc_curve(tpr, fpr, scatter = True, ax = None):
        '''
        Plots the ROC Curve by using the list of coordinates (tpr and fpr).

        Args:
            tpr: The list of TPRs representing each coordinate.
            fpr: The list of FPRs representing each coordinate.
            scatter: When True, the points used on the calculation will be plotted with the line (default = True).
        '''
        # if ax == None:
        #     plt.figure(figsize = (5, 5))
        #     ax = plt.axes()

        # if scatter:
        #     sns.scatterplot(x = fpr, y = tpr, ax = ax)
        # sns.lineplot(x = fpr, y = tpr, ax = ax)
        # sns.lineplot(x = [0, 1], y = [0, 1], color = 'green', ax = ax)
        # plt.xlim(-0.05, 1.05)
        # plt.ylim(-0.05, 1.05)
        # plt.xlabel("False Positive Rate")
        # plt.ylabel("True Positive Rate")

    # plt.figure(figsize = (12, 8))
    bins = [i/20 for i in range(20)] + [1]
    classes = [0,1,2]
    roc_auc_ovr = {}
    for i in range(len(classes)):
        # Gets the class
        c = classes[i]

        # Prepares an auxiliar dataframe to help with the plots
        df_aux = df.copy()
        # df={}
        # print(classes[i],ytest[0])
        df_aux['class'] = [1 if y == c else 0 for y in ytest]
        df_aux['prob'] = np.array(newArray)[:, i]
        df_aux = df_aux.reset_index(drop = True)

        # Plots the probability distribution for the class and the rest
        # ax = plt.subplot(2, 3, i+1)
        # sns.histplot(x = "prob", data = df_aux, hue = 'class', color = 'b', ax = ax, bins = bins)
        # ax.set_title(c)
        # ax.legend([f"Class: {c}", "Rest"])
        # ax.set_xlabel(f"P(x = {c})")

        # Calculates the ROC Coordinates and plots the ROC Curves
        # ax_bottom = plt.subplot(2, 3, i+4)
        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])
        # plot_roc_curve(tpr, fpr, scatter = False )
        # ax_bottom.set_title("ROC Curve OvR")

        # Calculates the ROC AUC OvR
        roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])
    # plt.tight_layout()


    # Displays the ROC AUC for each class
    avg_roc_auc = 0
    i = 0
    class_dict={0:'FA',1:'NIFTP', 2:'PTC'}
    roc_auc_str=''
    for k in roc_auc_ovr:
        avg_roc_auc += roc_auc_ovr[k]
        i += 1
        print(f"{k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}")
        roc_auc_str=roc_auc_str+class_dict[k]+' - '+f"{roc_auc_ovr[k]:.4f}"+"  "

    results_dict['Class-wise ROC_AUC_n score']=roc_auc_str
    print(f"average ROC AUC OvR: {avg_roc_auc/i:.4f}")
    results_dict['Average ROC_AUC_n score']=f"{avg_roc_auc/i:.4f}"
    print(results_dict)



    # Displays the ROC AUC for each class
    avg_roc_auc = 0
    i = 0
    class_dict={0:'FA',1:'NIFTP', 2:'PTC'}
    roc_auc_str=''
    for k in roc_auc_ovr:
        avg_roc_auc += roc_auc_ovr[k]
        i += 1
        print(f"{k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}")
        roc_auc_str=roc_auc_str+class_dict[k]+' - '+f"{roc_auc_ovr[k]:.4f}"+"  "

    results_dict['Class-wise ROC_AUC_n score']=roc_auc_str
    print(f"average ROC AUC OvR: {avg_roc_auc/i:.4f}")
    results_dict['Average ROC_AUC_n score']=f"{avg_roc_auc/i:.4f}"

    macro_roc_auc_func_n = roc_auc_score(ytest, newArray, labels = classes, multi_class = 'ovr', average = 'macro')
    print('macro average roc_auc_n score from function - ', macro_roc_auc_func_n)
    weighted_roc_auc_func_n = roc_auc_score(ytest, newArray, labels = classes, multi_class = 'ovr', average = 'weighted')
    print('weighted average roc_auc_n score from function - ', weighted_roc_auc_func_n)
    macro_ovo_roc_auc_func_n = roc_auc_score(ytest, newArray, labels = classes, multi_class = 'ovo', average = 'macro')
    print('macro average ovo roc_auc_n score from function - ', macro_ovo_roc_auc_func_n)
    weighted_ovo_roc_auc_func_n = roc_auc_score(ytest, newArray, labels = classes, multi_class = 'ovo', average = 'weighted')
    print('weighted average ovo roc_auc score_n from function - ', weighted_ovo_roc_auc_func_n)

    results_dict['Macro Average ROC_AUC_n score'] = macro_roc_auc_func_n
    results_dict['Weighted Average ROC_AUC_n score'] = weighted_roc_auc_func_n
    results_dict['Macro Average ovo ROC_AUC_n score'] = macro_roc_auc_func_n
    results_dict['Weighted Average ovo ROC_AUC_n score'] = weighted_roc_auc_func_n

    print(results_dict)


    import pandas as pd
    now= datetime.now()
    dt_string = now.strftime("%d_%m_%Y %H_%M_%S")
    results_df=pd.DataFrame(results_dict, index=[1])
    results_df.to_excel('/workspace/storage/results/densenet/densenet_nodule_results'+dt_string+'.xlsx')

    # ovo_roc_auc_func = roc_auc_score(ytest, prob, labels = classes, multi_class = 'ovo', average = None)
    # print('ovo roc_auc score from function - ', macro_ovo_roc_auc_func)
    # ovo_roc_auc_func_n = roc_auc_score(ytest, newArray, labels = classes, multi_class = 'ovo', average = None)
    # print('ovo roc_auc score_n from function - ', weighted_ovo_roc_auc_func)

    # results_dict['ovo ROC_AUC score'] = ovo_roc_auc_func
    # results_dict['ovo ROC_AUC_n score'] = ovo_roc_auc_func_n
    # results_df=pd.DataFrame(results_dict, index=[1])
    # results_df.to_excel('/workspace/storage/results/densenet/densenet_capsule_results'+dt_string+'.xlsx')